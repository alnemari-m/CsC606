---
layout: page
title: Project
permalink: /project/
---
# Project Ideas for Computer Vision Course

<table style="border-collapse: collapse; width: 100%;">
  <tr style="background-color: #ADD8E6; color: #000000;">
    <td><strong>Title</strong></td>
    <td><strong>Description</strong></td>
    <td><strong>Key Topics Covered</strong></td>
    <td><strong>Datasets</strong></td>
  </tr>
  <tr style="background-color: #FFFFFF; color: #000000;">
    <td><strong>Multi-View Geometry and 3D Reconstruction</strong></td>
    <td>Implement a 3D reconstruction pipeline using images captured from multiple viewpoints.</td>
    <td>
      <ul style="margin: 0; padding: 10px;">
        <li>Camera Models</li>
        <li>Feature Detection</li>
        <li>Triangulation</li>
        <li>Bundle Adjustment</li>
      </ul>
    </td>
    <td>
      <ul style="margin: 0; padding: 10px;">
        <li><a href="https://vision.middlebury.edu/stereo/">Middlebury Stereo</a> (~1 GB)</li>
        <li><a href="https://colmap.github.io/datasets.html">COLMAP Dataset</a> (~5 GB)</li>
        <li><a href="https://www.doc.ic.ac.uk/~ahanda/VaFRIC/iclnuim.html">ICL-NUIM</a> (~7 GB)</li>
        <li><a href="https://cvlabwww.epfl.ch/data/rgbd/tsukuba.php">New Tsukuba Dataset</a> (~4 GB)</li>
        <li><a href="https://www.eth3d.net/datasets">ETH3D (Low-Res Subset)</a> (~2 GB)</li>
      </ul>
    </td>
  </tr>
  <tr style="background-color: #90EE90; color: #000000;">
    <td><strong>Object Detection and Recognition using Deep Learning</strong></td>
    <td>Develop an object detection system using CNN frameworks like YOLO or Faster R-CNN.</td>
    <td>
      <ul style="margin: 0; padding: 10px;">
        <li>Convolutional Neural Networks (CNNs)</li>
        <li>Object Detection</li>
        <li>Transfer Learning</li>
        <li>Performance Evaluation</li>
      </ul>
    </td>
    <td>
      <ul style="margin: 0; padding: 10px;">
        <li><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/">Pascal VOC 2012</a> (~2 GB)</li>
        <li><a href="https://github.com/Shenggan/BCCD_Dataset">BCCD Dataset</a> (~120 MB)</li>
        <li><a href="http://cvrr.ucsd.edu/LISA/lisa-traffic-sign-dataset.html">LISA Traffic Light Dataset</a> (~2 GB)</li>
        <li><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2007/">VOC2007</a> (~400 MB)</li>
        <li><a href="https://captain-whu.github.io/DOTA/">DOTA (Subset)</a> (~8 GB)</li>
      </ul>
    </td>
  </tr>
  <tr style="background-color: #FFB6C1; color: #000000;">
    <td><strong>Image Segmentation with Conditional Random Fields (CRFs)</strong></td>
    <td>Implement an image segmentation system that uses CRFs as a post-processing step for refinement.</td>
    <td>
      <ul style="margin: 0; padding: 10px;">
        <li>Image Segmentation</li>
        <li>Conditional Random Fields (CRFs)</li>
        <li>Optimization Techniques</li>
        <li>Neural Networks for Segmentation</li>
      </ul>
    </td>
    <td>
      <ul style="margin: 0; padding: 10px;">
        <li><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/">Pascal VOC 2012</a> (~2 GB)</li>
        <li><a href="http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/">CamVid</a> (~5 GB)</li>
        <li><a href="https://mmcheng.net/msra10k/">MSRA10K</a> (~2 GB)</li>
        <li><a href="http://dags.stanford.edu/projects/scenedataset.html">Stanford Background Dataset</a> (~100 MB)</li>
        <li><a href="https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html">BSDS500</a> (~300 MB)</li>
      </ul>
    </td>
  </tr>
  <tr style="background-color: #ADD8E6; color: #000000;">
    <td><strong>Super-Resolution Imaging with GANs</strong></td>
    <td>Create a super-resolution model using a Generative Adversarial Network (GAN).</td>
    <td>
      <ul style="margin: 0; padding: 10px;">
        <li>Generative Adversarial Networks (GANs)</li>
        <li>Super-Resolution Techniques</li>
        <li>Image Quality Assessment</li>
        <li>Loss Functions in GANs</li>
      </ul>
    </td>
    <td>
      <ul style="margin: 0; padding: 10px;">
        <li><a href="https://data.vision.ee.ethz.ch/cvl/DIV2K/">DIV2K</a> (~7 GB)</li>
        <li><a href="https://github.com/scarlet-jimmy/LP-KPN/tree/master/data/benchmark/Set5">Set5</a> (~5 MB)</li>
        <li><a href="https://github.com/jbhuang0604/SelfExSR/tree/master/data/Urban100">Urban100</a> (~250 MB)</li>
        <li><a href="http://www.manga109.org/en/download.html">Manga109</a> (~5 GB)</li>
        <li><a href="https://github.com/scarlet-jimmy/LP-KPN/tree/master/data/benchmark/Set14">Set14</a> (~6 MB)</li>
      </ul>
    </td>
  </tr>
  <tr style="background-color: #90EE90; color: #000000;">
    <td><strong>Structure-from-Motion (SfM) and SLAM</strong></td>
    <td>Develop a system for 3D map reconstruction and real-time localization using SfM and SLAM.</td>
    <td>
      <ul style="margin: 0; padding: 10px;">
        <li>Structure-from-Motion (SfM)</li>
        <li>Simultaneous Localization and Mapping (SLAM)</li>
        <li>Camera Pose Estimation</li>
        <li>Point Cloud Processing</li>
      </ul>
    </td>
    <td>
      <ul style="margin: 0; padding: 10px;">
        <li><a href="https://www.doc.ic.ac.uk/~ahanda/VaFRIC/iclnuim.html">ICL-NUIM</a> (~7 GB)</li>
        <li><a href="https://cvlabwww.epfl.ch/data/rgbd/tsukuba.php">New Tsukuba Dataset</a> (~4 GB)</li>
        <li><a href="https://robotcar-dataset.robots.ox.ac.uk/">Oxford RobotCar Dataset (Small Subset)</a> (~5 GB)</li>
        <li><a href="https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets">EuRoC MAV Dataset</a> (~5 GB)</li>
        <li><a href="https://www.eth3d.net/datasets">ETH3D SLAM Benchmark (Low-Res Subset)</a> (~2 GB)</li>
      </ul>
    </td>
  </tr>
  <tr style="background-color: #FFB6C1; color: #000000;">
    <td><strong>Panoptic Segmentation for Urban Scenes</strong></td>
    <td>Implement a panoptic segmentation model that combines instance and semantic segmentation.</td>
    <td>
      <ul style="margin: 0; padding: 10px;">
        <li>Semantic Segmentation</li>
        <li>Instance Segmentation</li>
        <li>Panoptic Segmentation</li>
        <li>Deep Learning Architectures for Segmentation</li>
      </ul>
    </td>
    <td>
      <ul style="margin: 0; padding: 10px;">
        <li><a href="https://www.cityscapes-dataset.com/">Cityscapes</a> (~5 GB subset)</li>
        <li><a href="http://groups.csail.mit.edu/vision/datasets/ADE20K/">ADE20K</a> (~2 GB)</li>
        <li><a href="https://www.mapillary.com/dataset/vistas">Mapillary Vistas</a> (~5 GB subset)</li>
        <li><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/">Pascal VOC 2012</a> (~2 GB)</li>
        <li><a href="http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/">CamVid</a> (~5 GB)</li>
      </ul>
    </td>
  </tr>
  <tr style="background-color: #ADD8E6; color: #000000;">
    <td><strong>Pose Estimation for Human Motion Analysis</strong></td>
    <td>Develop a system to estimate human poses from images or videos for action recognition or tracking.</td>
    <td>
      <ul style="margin: 0; padding: 10px;">
        <li>Pose Estimation</li>
        <li>Human Motion Analysis</li>
        <li>Deep Learning for Vision</li>
        <li>Action Recognition</li>
      </ul>
    </td>
    <td>
      <ul style="margin: 0; padding: 10px;">
        <li><a href="https://cocodataset.org/#keypoints-2019">COCO Keypoints</a> (~7 GB)</li>
        <li><a href="http://human-pose.mpi-inf.mpg.de/">MPII Human Pose Dataset</a> (~12 GB, use subset)</li>
        <li><a href="http://www.comp.leeds.ac.uk/mat4saj/lsp.html">LSP Dataset</a> (~200 MB)</li>
        <li><a href="http://dreamdragon.github.io/PennAction/">Penn Action Dataset</a> (~1 GB)</li>
        <li><a href="http://vision.imar.ro/human3.6m/">Human3.6M (subset)</a> (~5 GB subset)</li>
      </ul>
    </td>
  </tr>
  <tr style="background-color: #90EE90; color: #000000;">
    <td><strong>Optical Flow Estimation for Motion Tracking</strong></td>
    <td>Create a system that estimates optical flow between video frames to track motion.</td>
    <td>
      <ul style="margin: 0; padding: 10px;">
        <li>Optical Flow Estimation</li>
        <li>Motion Tracking</li>
        <li>Video Processing</li>
        <li>Deep Learning for Optical Flow</li>
      </ul>
    </td>
    <td>
      <ul style="margin: 0; padding: 10px;">
        <li><a href="http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow">KITTI Optical Flow 2012</a> (~4 GB)</li>
        <li><a href="http://sintel.is.tue.mpg.de/">Sintel</a> (~6 GB)</li>
        <li><a href="https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html">Flying Chairs Dataset</a> (~2 GB)</li>
        <li><a href="http://vision.middlebury.edu/flow/data/">Middlebury Optical Flow</a> (~1 GB)</li>
        <li><a href="https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html">FlyingThings3D (subset)</a> (~5 GB subset)</li>
      </ul>
    </td>
  </tr>
  <tr style="background-color: #FFB6C1; color: #000000;">
    <td><strong>Monocular Depth Estimation Using CNNs</strong></td>
    <td>Implement a monocular depth estimation system using a convolutional neural network (CNN).</td>
    <td>
      <ul style="margin: 0; padding: 10px;">
        <li>Depth Estimation</li>
        <li>Monocular Vision</li>
        <li>Convolutional Neural Networks (CNNs)</li>
        <li>3D Vision</li>
      </ul>
    </td>
    <td>
      <ul style="margin: 0; padding: 10px;">
        <li><a href="https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html">NYU Depth V2</a> (~5 GB)</li>
        <li><a href="http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction">KITTI Depth Prediction</a> (~3 GB)</li>
        <li><a href="http://make3d.cs.cornell.edu/data.html">Make3D</a> (~2 GB)</li>
        <li><a href="https://www.eth3d.net/datasets">ETH3D (Low-Res Subset)</a> (~2 GB)</li>
        <li><a href="http://rgbd.cs.princeton.edu/">SUN RGB-D</a> (~6 GB)</li>
      </ul>
    </td>
  </tr>
  <tr style="background-color: #ADD8E6; color: #000000;">
    <td><strong>Image Inpainting for Scene Completion</strong></td>
    <td>Develop an image inpainting system to fill in missing or corrupted parts of an image.</td>
    <td>
      <ul style="margin: 0; padding: 10px;">
        <li>Image Inpainting</li>
        <li>Generative Models</li>
        <li>Deep Learning for Image Synthesis</li>
        <li>Image Restoration</li>
      </ul>
    </td>
    <td>
      <ul style="margin: 0; padding: 10px;">
        <li><a href="https://github.com/tkarras/progressive_growing_of_gans">CelebA-HQ</a> (~5 GB)</li>
        <li><a href="http://places2.csail.mit.edu/download.html">Places2</a> (~7 GB subset)</li>
        <li><a href="http://www.vision.ee.ethz.ch/~qianyan/ParisDataset/">Paris StreetView</a> (~2 GB)</li>
        <li><a href="http://www.image-net.org/">ImageNet (subset)</a> (~10 GB subset)</li>
        <li><a href="https://cocodataset.org/#home">COCO (subset)</a> (~5 GB subset)</li>
      </ul>
    </td>
  </tr>
</table>

# Project Timeline and Expectations

<table style="border-collapse: collapse; width: 100%;">
  <tr style="background-color: #ADD8E6; color: #000000;">
    <td><strong>Week 3</strong></td>
    <td><strong>Phase 1: Proposals Phase</strong></td>
    <td>
      <ul style="margin: 0; padding: 10px;">
        <li><strong>Research and Data Collection:</strong> Identify and gather all necessary datasets or resources critical for the project. This includes images, videos, or any specific data required for analysis.</li>
        <li><strong>Problem Definition:</strong> Articulate a clear and concise problem statement. Define the key challenges and objectives that the project aims to address.</li>
        <li><strong>Preliminary Analysis:</strong> Conduct an initial feasibility study. Test basic algorithms or models on a small subset of your data to ensure that the project is viable.</li>
        <li><strong>Report and Presentation:</strong> Prepare a comprehensive report detailing your findings. The report should cover the motivation behind the project, related work, and the problem definition. Present your initial findings to gather feedback and refine your approach.</li>
      </ul>
    </td>
  </tr>
  <tr style="background-color: #90EE90; color: #000000;">
    <td><strong>Week 10</strong></td>
    <td><strong>Phase 2: Progress Report</strong></td>
    <td>
      <ul style="margin: 0; padding: 10px;">
        <li><strong>System Design and Implementation:</strong> Develop and implement the core components of your system. This phase involves rigorous coding and system integration based on your initial plans.</li>
        <li><strong>Preliminary Results:</strong> Test your system with a subset of the data and analyze the preliminary results. Identify any issues or areas needing improvement.</li>
        <li><strong>Presentation:</strong> Create a detailed progress presentation that highlights your system's design, the implementation process, and the preliminary results. Discuss any challenges faced and how you plan to overcome them in the final phase.</li>
      </ul>
    </td>
  </tr>
  <tr style="background-color: #FFB6C1; color: #000000;">
    <td><strong>Week 15</strong></td>
    <td><strong>Phase 3: Final Report</strong></td>
    <td>
      <ul style="margin: 0; padding: 10px;">
        <li><strong>Complete Implementation:</strong> Finalize your project by refining and optimizing the system. Incorporate feedback from the previous phases and ensure that the system is fully functional.</li>
        <li><strong>Results and Discussion:</strong> Perform a thorough evaluation of your system using the full dataset. Analyze the results comprehensively, discussing both the quantitative and qualitative aspects. Reflect on the strengths and limitations of your approach.</li>
        <li><strong>Final Report and Presentation:</strong> Compile a detailed final report that encapsulates the entire project journey, from the initial proposal to the final results. Prepare and deliver a polished presentation that effectively communicates your work, its outcomes, and potential future directions.</li>
      </ul>
    </td>
  </tr>
</table>
